{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Advanced dimensionality reduction techniques\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this train, we'll investigate how PCA, MDS and t-SNE dimensionality reduction techniques work on image and text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to;\n",
    "- Understand advanced dimensionality reduction techniques.\n",
    "- Implement these techniques on image and text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19459071",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "In this train we will:\n",
    "* Explain the following advanced dimensionality reduction techniques:\n",
    "    * Principal component analysis\n",
    "    * Multi-dimensional scaling\n",
    "    * t-SNE\n",
    "* Implement these techniques on an image dataset.\n",
    "* Implement these techniques on a text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5ec89",
   "metadata": {},
   "source": [
    "## Dimensionality reduction on images\n",
    "This train will provide a high-level overview of some of the more advanced techniques for dimensionality reduction. We will be using the [handwritten digits data set](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), which is a collection of images of handwritten digits between 0 and 9. The data was generated by a total of 43 people, who wrote a total of 5620 digits by hand which were then digitised and processed into 8x8 greyscale images.\n",
    "\n",
    "**Note**: Most of the code has been taken directly from [this example code](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#), which contains a few other examples of dimensionality reduction techniques in addition to those expanded on here.\n",
    "\n",
    "We'll start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, preprocessing)\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8c9c3",
   "metadata": {},
   "source": [
    "### Loading and inspecting the data\n",
    "The digits dataset is included as part of the `sklearn` library, which means loading it into our notebook is a breeze. For this train, we will simplify things by only looking at the first six digits in the dataset. We can use the `n_class` argument in the `load_digits()` function to select only the numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(\"Total number of samples: \", n_samples)\n",
    "print(\"Features per sample: \", n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac978fb6",
   "metadata": {},
   "source": [
    "Below, the image shows a selection from the 64-dimensional digits dataset. Each digit is represented as an 8x8 array of pixels, with values ranging between 0 (white) and 16 (black).  \n",
    "\n",
    "![digits dataset](https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_001.png)   \n",
    "Source: [Load digits dataset](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)\n",
    "\n",
    "The same concept applies to our dataset, only the arrays are a little smaller (8x8) and they have been strung out into single rows of 64 values, ranging between 0 and 16. Let's take a look at the first digit in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d82738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What digit is this?\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8405c",
   "metadata": {},
   "source": [
    "Could you tell that the first array was a zero? Unless you are some sort of savant genius, you probably wouldn't be able to tell which digit is represented by looking only at the array values.   \n",
    "\n",
    "Luckily, we can use the `imshow()` function from the matplotlib library to plot these digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images of the digits\n",
    "n_img_per_row = 20\n",
    "\n",
    "# create a large grid of zeros\n",
    "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
    "\n",
    "# for each column\n",
    "for i in range(n_img_per_row):\n",
    "    \n",
    "    # find the x-coordinate\n",
    "    ix = 10 * i + 1\n",
    "    \n",
    "    # for each row \n",
    "    for j in range(n_img_per_row):\n",
    "        \n",
    "        # find the y-coordinate\n",
    "        iy = 10 * j + 1\n",
    "        \n",
    "        # change the pixels in this part of the grid to match the digit\n",
    "        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
    "\n",
    "# plot the grid as a single image\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('A selection from the 64-dimensional digits dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3412090",
   "metadata": {},
   "source": [
    "### Dimensionality reduction techniques\n",
    "Now what we are going to try and do is reduce the dimensionality of the entire dataset to just two dimensions (down from 64 dimensions – one per pixel). We will use a few different dimensionality reduction techniques, and at each stage, we will plot the data and see how it is distributed in these two dimensions.   \n",
    "\n",
    "What is very important to note here is that none of these algorithms will be shown the labels of the data; they will be entirely unsupervised. In each case, we will plot the data in two dimensions, but include the known labels of the data points in the plots for our own validation.   \n",
    "\n",
    "For plotting, we are going to use the same `plot_embedding()` function defined in the original `sklearn` example code. This does a fantastic job of plotting the digits in a two-dimensional space, so there is no need to reinvent the wheel here.   \n",
    "\n",
    "**Note:** You will see the word **\"embedding\"** used in some of the comments and plots below. Simply put, an embedding is a representation of a vector in a different feature space. So in this case, the original digits exist as 64-dimensional arrays and are then reduced to just two dimensions. The resulting two-dimensional vectors are referred to as the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualise the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    \n",
    "    # normalise data\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746b37f",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "PCA was covered in a previous train, so we won't go into great detail about it here. We know that the objective of PCA is to decompose a dataset into mutually orthogonal components that **each maximise the variance in the dataset**.   \n",
    "\n",
    "We will use PCA to decompose the dataset into the first two principal components, which will contain the largest and second-largest amounts of variance, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ed492",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PCA projection\")\n",
    "t0 = time()\n",
    "X_pca = decomposition.PCA(n_components=2).fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Finished PCA projection in \" + str(t1-t0) + \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5eeef1",
   "metadata": {},
   "source": [
    "Now, before using the awesome `plot_embedding()` function, let's plot the data, without labels, in the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1],\n",
    "                     sizes=(10, 200))\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0780e1",
   "metadata": {},
   "source": [
    "Can you spot any obvious clusters? Perhaps one or two on the right side of the plot?   \n",
    "\n",
    "Now, let's plot the data with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_pca,\n",
    "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d4a11",
   "metadata": {},
   "source": [
    "So we can see that digits do seem to group together, but there isn't the clearest separation between the different digits.   \n",
    "\n",
    "Now, let's take a look at some other techniques and see how they perform.   \n",
    "\n",
    "### Multi-dimensional scaling (MDS)\n",
    "The goal of MDS is to map features to a low-dimensional space **while preserving the distances** between observations in a given dataset.   \n",
    "\n",
    "MDS can be performed using algorithms that are either *metric* or *non-metric*. Non-metric approaches are typically used to preserve ordinality (order) within data. This is more of a necessity when there are categorical features present in the data. Since the data used here is entirely numeric, we will use a metric approach.   \n",
    "\n",
    "The **stress** is a measure of the degree to which distances between points in the original feature space correspond with the distances in the low-dimensional space. A lower stress value is preferred, and it is this quantity that is minimised by MDS.\n",
    "\n",
    "For more information on MDS, read the [`sklearn` user guide](https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, \n",
    "                   n_init=4, \n",
    "                   max_iter=200,\n",
    "                   n_jobs=-1,\n",
    "                   random_state=42,\n",
    "                   dissimilarity='euclidean')\n",
    "t0 = time()\n",
    "X_mds = clf.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Done. Stress: %f\" % clf.stress_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_mds,\n",
    "               \"MDS embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffa19a",
   "metadata": {},
   "source": [
    "### t-distributed stochastic neighbor embedding (t-SNE)\n",
    "t-SNE is a very complex technique, which can often yield truly stunning results when reducing high-dimensional datasets. Here is a pretty good explanation of t-SNE from [Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding):   \n",
    "> \"The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimises the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\"   \n",
    "\n",
    "Additionally, you can watch the video below for a more detailed explanation of how t-SNE works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6272a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Youtube\n",
    "IFrame(width=\"840\", height=\"472\", src=\"https://www.youtube.com/embed/NEaUSP4YerM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bd618",
   "metadata": {},
   "source": [
    "We won't get into the finer details here. Instead, here is a summary of the important points of t-SNE:   \n",
    "- Non-linear transformation\n",
    "- Preserves both local and global structure (usually)\n",
    "- Computationally expensive\n",
    "\n",
    "For more information on t-SNE, check out the [`sklearn` user guide](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne) or read through any of the articles below:\n",
    "\n",
    "- **O'Reilly**: [An illustrated introduction to the t-SNE algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)\n",
    "- **Analytics Vidhya**: [Comprehensive guide on t-SNE algorithm with implementation in R & Python](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n",
    "- **DataCamp**: [Introduction to t-SNE](https://www.datacamp.com/community/tutorials/introduction-t-sne)\n",
    "  \n",
    "Let's fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2370f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2,\n",
    "                     perplexity=40,\n",
    "                     metric='euclidean',\n",
    "                     init='pca',\n",
    "                     verbose=1,\n",
    "                     random_state=42)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_tsne,\n",
    "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992825e7",
   "metadata": {},
   "source": [
    "Wow, how cool is that?!   \n",
    "\n",
    "Remember, the algorithm made no use of the labels. The digit arrays were reduced into a two-dimensional space where similar digits ended up close together. We have six main clusters – one for each digit class. But we also have a few smaller clusters – notice the small group of twos that are somewhere between the 1s and the rest of the 2s? Or the 1s that are quite close to the 2s?   \n",
    "\n",
    "### Word of caution in t-SNE\n",
    "In this case, we have demonstrated the power of t-SNE as a tool for exploratory data analysis and to reveal natural groups, or clusters, within datasets.   \n",
    "\n",
    "However, t-SNE can also be very misleading at times and you are encouraged to investigate the effects of the various hyperparameters of the results when working with different datasets.   \n",
    "\n",
    "Check out [this article](https://distill.pub/2016/misread-tsne/) for some insights into t-SNE's various hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ce557",
   "metadata": {},
   "source": [
    "## Dimensionality reduction on text\n",
    "\n",
    "We'll now work through the same dimensionality reduction techniques again, but this time using text data.\n",
    "\n",
    "### Loading and inspecting the data\n",
    "The data we'll be using are Twitter data from @CapeTownFreeway, posted during the year 2017. Let's import the NLP packages we'll need and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/unsupervised_sprint/capetownfreeway.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31908b71",
   "metadata": {},
   "source": [
    "Now for some preprocessing. A few of these columns will not be useful to us going forward, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['source', 'in_reply_to_screen_name', 'is_retweet'],\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653b7c5",
   "metadata": {},
   "source": [
    "We'll define a function called **clean_tweet** to remove any URLs and any campaign-specific keywords that are frequently repeated in tweets. These tend to create a large amount of noise in the data, which will make the clustering process difficult later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    no_link_loc = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    no_num_loc = no_link_loc.lower()\n",
    "    for c in ['inbound','outbound','after','before','update','after','animals','roadworks',':',',',\n",
    "              '#','@','savewater', 'boozefreeroads','speedkillsfacts','saferoadsforall', 'sharetheroad',\n",
    "              'alwaysbuckleup', 'boozefreeroad','alwaysbuckleup','savekidslives','.','walksafe']:\n",
    "        no_num_loc = no_num_loc.replace(c, '')\n",
    "        \n",
    "    no_num_loc = no_num_loc.split(',')[0]\n",
    "    try:\n",
    "        return no_num_loc\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['text'].apply(clean_tweet)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1e427",
   "metadata": {},
   "source": [
    "To see the effect of cleaning our tweets, we quickly view a wordcloud illustration of the data. You may need to download the required `nltk` resource specified below. If so, uncomment the code and run it before you continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38123cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87828b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in df.clean_tweet:\n",
    "    words.extend(word_tokenize(i))\n",
    "    \n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(frequencies = Counter(words))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db933c",
   "metadata": {},
   "source": [
    "As you can see, the ratio of 'delays' to 'cleared' is a clear eyesore for the traffic situation in Cape Town, with clear dominance from the N1 and N2 motorways into the city. \n",
    "\n",
    "Before we move on, we need to vectorize our text. Let's do that using  the term frequency–inverse document frequency technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdeeef9",
   "metadata": {},
   "source": [
    "### Term frequency–inverse document frequency (tf–idf)\n",
    "**Term frequency (tf)** measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear far more often in long documents than shorter ones. Thus, the term frequency is often divided by the document length (a.k.a. the total number of terms in the document) as a way of normalisation: \n",
    "\n",
    "TF($t$) = (Number of times term $t$ appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "<i>(This is the relative frequency of each term in a document)</i>\n",
    "\n",
    "**Inverse document frequency (idf)** measures how important a term is. While computing tf, all terms are considered equally important. However, it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear many times but have little importance. Thus, we need to down-weight the frequent terms while up-weighting the rare ones, by computing the following: \n",
    "\n",
    "IDF($t$) = ln(Total number of documents / Number of documents containing term $t$).\n",
    "\n",
    "**tf–idf** is the product of the values of **tf** and **idf**.\n",
    "\n",
    "\n",
    "**An example**\n",
    "> Consider a document containing 100 words wherein the word \"cat\" appears three times. The term frequency (i.e. tf) for \"cat\" is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word \"cat\" appears in one thousand of these. Then, the inverse document frequency (i.e. idf) is calculated as ln(10,000,000 / 1,000) = 4. Thus, the tf–idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "More information can be found here: http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/.\n",
    "\n",
    "Luckily for us, there is a handy `sklearn` package called `TfidfVectorizer` that we can use to choose the number of features we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=50, stop_words='english')\n",
    "X = vectorizer.fit_transform(df.clean_tweet).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88926b6f",
   "metadata": {},
   "source": [
    "The tf–idf vectorizer creates a feature set of length 50, with the most frequently used words in the tweets we provided. It takes care of English stopwords such as \"in\", \"on\", \"the\", etc. which carry little to no meaning in terms of the content of the tweet and helps us to remove a large amount of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=vocab)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713680e",
   "metadata": {},
   "source": [
    "So we are now at a stage where we have tf–idf vectors for each tweet, constructed from the 50 most popular words in the corpus. Let's now implement PCA, MDS, and t-SNE on these vectors.\n",
    "\n",
    "### Principal component analysis (PCA)\n",
    "We will use PCA to decompose the dataset into the first two principal components, which will contain the largest and second-largest amounts of variance, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44146e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "print(\"Computing PCA projection\")\n",
    "t0 = time()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "t1 = time()\n",
    "print(\"Finished PCA projection in \" + str(t1-t0) + \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1])\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "plt.show()\n",
    "print(\"The first two principal components explain \" + \n",
    "      str(np.round(sum(pca.explained_variance_ratio_[:2]),2)*100) + \" % of the variance in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48808d",
   "metadata": {},
   "source": [
    "In the images example above, we created the function `plot_embedding` to help us visualise what the clusters represent. In this example, let's make it possible for us to view what text each data point portrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['text'] = df['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x = pca_df.iloc[:2000,0].values,\n",
    "        y = pca_df.iloc[:2000,1].values,\n",
    "        text = pca_df.iloc[:2000,2].values,\n",
    "        hoverinfo = 'text',\n",
    "        marker = dict(\n",
    "            color = 'lightblue'\n",
    "        ),\n",
    "        mode='markers',\n",
    "        showlegend = False\n",
    "    )\n",
    "]\n",
    "\n",
    "iplot(data, filename = \"add-hover-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e1b4b",
   "metadata": {},
   "source": [
    "Hover over the data points to view the text that data point represents. Can you spot any obvious clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca513b",
   "metadata": {},
   "source": [
    "### Multi-dimensional scaling (MDS)\n",
    "\n",
    "Let's do the same as above, but using MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "print(\"Computing MDS embedding\")\n",
    "mds = manifold.MDS(n_components=2, \n",
    "                   n_init=2, \n",
    "                   max_iter=200,\n",
    "                   n_jobs=-1,\n",
    "                   random_state=42,\n",
    "                   dissimilarity='euclidean')\n",
    "t0 = time()\n",
    "X_mds = mds.fit_transform(X.iloc[:2000,:])\n",
    "t1 = time()\n",
    "print(\"Done. Stress: %f\" % mds.stress_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_df = pd.DataFrame(X_mds, columns=['D1', 'D2'])\n",
    "mds_df['text'] = df['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf37199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x = mds_df.iloc[:2000,0].values,\n",
    "        y = mds_df.iloc[:2000,1].values,\n",
    "        text = mds_df.iloc[:2000,2].values,\n",
    "        hoverinfo = 'text',\n",
    "        marker = dict(\n",
    "            color = 'lightblue'\n",
    "        ),\n",
    "        mode='markers',\n",
    "        showlegend = False\n",
    "    )\n",
    "]\n",
    "\n",
    "iplot(data, filename = \"add-hover-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e8145",
   "metadata": {},
   "source": [
    "There are two important things we note about an MDS scatter plot:\n",
    "\n",
    "- The axes themselves are meaningless.\n",
    "- The orientation of the figure is arbitrary.\n",
    "    \n",
    "The important thing is the proximity of the points. Since this is a lower-dimensional representation of 'similarity' between points in the higher-dimensional space, it is a useful starting point for clustering and other techniques that look to group points together that share similar characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddac953",
   "metadata": {},
   "source": [
    "### t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "t-SNE often yields truly stunning results when reducing high-dimensional datasets. Let's compute the t-SNE embedding for our text data and see what clusters stand out to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626185a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2,\n",
    "                     perplexity=40,\n",
    "                     metric='euclidean',\n",
    "                     init='pca',\n",
    "                     verbose=1,\n",
    "                     random_state=42)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X.iloc[:5000,:])\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(X_tsne, columns=['D1', 'D2'])\n",
    "tsne_df['text'] = df['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9299a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x = tsne_df.iloc[:2000,0].values,\n",
    "        y = tsne_df.iloc[:2000,1].values,\n",
    "        text = tsne_df.iloc[:2000,2].values,\n",
    "        hoverinfo = 'text',\n",
    "        marker = dict(\n",
    "            color = 'lightblue'\n",
    "        ),\n",
    "        mode='markers',\n",
    "        showlegend = False\n",
    "    )\n",
    "]\n",
    "\n",
    "iplot(data, filename = \"add-hover-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1eef",
   "metadata": {},
   "source": [
    "The same comments on MDS regarding orientation and values also apply to t-SNE. It seems that there are already some well-formed clusters in the above representation, suggesting that further investigation needs to be done on the nature of these associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a32e55",
   "metadata": {},
   "source": [
    "Can you determine why some of these clusters were formed? View the text in the clusters and see if you can figure out any similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb838cce",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This train provided a short recap of PCA and introduced MDS and t-SNE as methods of advanced dimensionality reduction. These concepts were demonstrated using two different datasets: one of images and another of text. You have seen how using these can be effective in reducing the dimensions and thus the computational complexity when working with various forms of data. Now it is up to you to experiment with the above example to gain a further understanding of dimensionality reduction techniques and their uses.\n",
    "\n",
    "## Additional links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e15f65",
   "metadata": {},
   "source": [
    "[A beginner’s guide to dimensionality reduction in Machine Learning](https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e)\n",
    "\n",
    "[Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
